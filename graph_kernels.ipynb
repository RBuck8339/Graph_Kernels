{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74dc9f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronan\\AppData\\Local\\Temp\\ipykernel_16264\\2850511496.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ronan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as acc, roc_auc_score as auc\n",
    "from sklearn import svm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential #\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dot, Embedding, Flatten, Dense\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.kernel_approximation import PolynomialCountSketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b399129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_node_labels(filename_nodes_to_graph, filename_node_labels, nr_graphs):\n",
    "    \n",
    "    with open(filename_nodes_to_graph) as f_nodes:\n",
    "        nodes = f_nodes.read().splitlines()\n",
    "        \n",
    "    with open(filename_node_labels) as f_labels:\n",
    "        labels = f_labels.read().splitlines()\n",
    "    \n",
    "    if (len(nodes) != len(labels)):\n",
    "        raise ValueError('Node lists of different length')\n",
    "        return -1\n",
    "    \n",
    "    Vs = [{} for _ in range(nr_graphs)]\n",
    "    nodes_to_graph = {}\n",
    "    node_labels = {}\n",
    "    set_labels = set()\n",
    "    for i in range(len(nodes)):\n",
    "        node_id = i+1\n",
    "        graph_id = int(nodes[i])\n",
    "        nodes_to_graph[node_id] = graph_id\n",
    "        label = labels[i]\n",
    "        set_labels.add(label)\n",
    "        node_labels[node_id] = label\n",
    "        Vs[graph_id][node_id] = label\n",
    "    return Vs, nodes_to_graph, node_labels, set_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569a4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edges(filename_edges, Vs, nodes_to_graph, node_labels, nr_graphs, sep=','):\n",
    "    Es = [{} for _ in range(nr_graphs)]\n",
    "    f_edges = open(filename_edges, 'r')\n",
    "    for line in f_edges: \n",
    "        line_split = str.split(line, sep)\n",
    "        e1 = int(line_split[0].strip())\n",
    "        e2 = int(line_split[1].strip())\n",
    "        if (nodes_to_graph[e1] != nodes_to_graph[e2]):\n",
    "            print('Vertices connected by and edge but belonging to different graphs')\n",
    "            print('nodes',  e1, e2)\n",
    "            print('graphs', nodes_to_graph[e1], nodes_to_graph[e2])\n",
    "        E = Es[nodes_to_graph[e1]]  \n",
    "        L = []\n",
    "        if e1 in E:\n",
    "            L = E[e1]\n",
    "        L.append(e2)\n",
    "        E[e1] = L\n",
    "        #Es[nodes_to_graph[e1]] = E\n",
    "    return Es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2b5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(folderpath, filename):\n",
    "    '''\n",
    "    A method for reading graphs in the format provided in \n",
    "    https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n",
    "    '''\n",
    "    \n",
    "    filename_edges = folderpath + '/' + filename + '/' + filename + '_A.txt'\n",
    "    filename_nodes_to_graph = folderpath + '/' + filename + '/' + filename + '_graph_indicator.txt'\n",
    "    filename_node_labels = folderpath + '/' + filename + '/' + filename + '_node_labels.txt'\n",
    "    filename_classes = folderpath + '/' + filename + '/' + filename + '_graph_labels.txt'\n",
    "    \n",
    "    print(filename_edges)\n",
    "    \n",
    "    classes = []\n",
    "    with open(filename_classes) as f_classes:\n",
    "        classes_f = f_classes.read().splitlines()  \n",
    "        for c in classes_f:\n",
    "            classes.append(int(c))\n",
    "        \n",
    "    nr_graphs = len(classes) + 1\n",
    "    Vs, nodes_to_graph, node_labels, set_labels = read_node_labels(filename_nodes_to_graph, filename_node_labels, nr_graphs)\n",
    "    Es = read_edges(filename_edges, Vs, nodes_to_graph, node_labels, nr_graphs)\n",
    "    \n",
    "    return Vs[1:], Es[1:], node_labels, classes, set_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0734a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/ronan/Downloads/AIDS/AIDS/AIDS_A.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/ronan/Downloads/AIDS\"\n",
    "filename = \"AIDS\"\n",
    "Vs, Es, node_labels, classes, set_labels = read_graph(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643dac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000, 2000, 38)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vs), len(Es), len(classes), len(set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4205d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, 0, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(classes), min(classes), max(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d8a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41f67039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31385"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6fd972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nx_graphs(Vs, Es):\n",
    "    '''\n",
    "    convert the nodes and edges into networkx graphs\n",
    "    '''\n",
    "    Gs = []\n",
    "    for i in range(len(Vs)):\n",
    "        V = Vs[i]\n",
    "        E = Es[i]\n",
    "        G = nx.Graph()\n",
    "        # G.add_nodes_from(V.keys())\n",
    "        for u,nbrs in E.items():\n",
    "            for v in nbrs:\n",
    "                G.add_edge(u, v)\n",
    "        Gs.append(G)\n",
    "    return Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f59694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = create_nx_graphs(Vs, Es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622a84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G, u, k, node_labels):\n",
    "    curr_node = u\n",
    "    walk = []\n",
    "    for i in range(k):\n",
    "        idx = random.randint(0,len(list(G[curr_node]))-1)\n",
    "        curr_node = list(G[curr_node])[idx]\n",
    "        walk.append(node_labels[curr_node])\n",
    "    return tuple(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c4ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a number of random walks per node\n",
    "# such that walks consist of the provided node labels \n",
    "nr_walks_per_node = 10\n",
    "walks_sets = {}\n",
    "nodes = set()\n",
    "for G in Gs:\n",
    "    for u in G.nodes():\n",
    "        for j in range(nr_walks_per_node):\n",
    "            walk = random_walk(G, u, 4, node_labels)\n",
    "            walks_sets.setdefault(walk, set())\n",
    "            walks_sets[walk].add(u)\n",
    "            nodes.add(u)\n",
    "walks = {}\n",
    "for w, ws in walks_sets.items():\n",
    "    walks[w] = list(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a1c13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87fa6648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879, 31175)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(walks), len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17d4e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityGen(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "        A generator class that will generate positive and negative node pairs.\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 nodes,\n",
    "                 walks,\n",
    "                 nr_neg_samples, \n",
    "                 batch_size):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        :nodes: the nodes in all graphs\n",
    "        :walks: a hash map with the nodes per walk\n",
    "        :param node_to_int: a mapping of node ids to consecutive integers\n",
    "        :param nr_neg_samples: the number of negative samples for positive pair\n",
    "        :param batch_size: how many samples to generate\n",
    "        \"\"\"\n",
    "        assert nr_neg_samples >= 1\n",
    "        self.nodes = nodes\n",
    "        self.walks = walks # the labeled walks for sampling of positive examples\n",
    "        # self.node_to_int = node_to_int\n",
    "        self.nr_neg_samples = nr_neg_samples # by how mach to multiply the number pf positive samples\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "         \n",
    "    # how many samples to generate per epoch        \n",
    "    def __len__(self):\n",
    "        return 20000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = 0\n",
    "        samples = []\n",
    "        labels = []\n",
    "        while i < self.batch_size:\n",
    "            i += 1\n",
    "            walk_idx = np.random.randint(len(self.walks))\n",
    "            walk = list(self.walks.keys())[walk_idx]\n",
    "            while len(self.walks[walk]) < 4:\n",
    "                walk_idx = np.random.randint(len(self.walks))\n",
    "                walk = list(self.walks.keys())[walk_idx]\n",
    "            u_idx = np.random.randint(len(self.walks[walk]))\n",
    "            v_idx = u_idx \n",
    "            while v_idx == u_idx:\n",
    "                v_idx = np.random.randint(len(self.walks[walk]))\n",
    "            \n",
    "            neg_idx = np.random.choice(len(nodes), self.nr_neg_samples)\n",
    "            neg = [self.nodes[idx] for idx in neg_idx]\n",
    "            \n",
    "            # generate positive samples from the random walk\n",
    "            samples.append((self.walks[walk][u_idx], self.walks[walk][v_idx]))\n",
    "            labels.append(1)\n",
    "            \n",
    "            # generate negative samples, a multiple of the walk length\n",
    "            samples.extend([(self.walks[walk][u_idx], n) for n in neg])\n",
    "            labels.extend(self.nr_neg_samples*[0])\n",
    "        return np.array(samples), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eeb5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(nodes)\n",
    "gen = SimilarityGen(nodes, walks, nr_neg_samples=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ebee5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[26107, 26105],\n",
       "        [26107, 19743],\n",
       "        [26107, 24012]]),\n",
       " array([1, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20befcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node2Vec(Model):\n",
    "    \"\"\"\n",
    "    The discriminative model that trains node embeddings. \n",
    "    Essentially, this is word2vec with negative sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_nodes, embedding_dim, *args, **kwargs):\n",
    "        super(Node2Vec, self).__init__(self, args, kwargs)\n",
    "        self.nr_nodes = nr_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.target_embedding = Embedding(nr_nodes,\n",
    "                                          embedding_dim,\n",
    "                                          embeddings_initializer=\"RandomNormal\",\n",
    "                                          input_length=1,\n",
    "                                          name=\"node_embedding\")\n",
    "        self.context_embedding = Embedding(nr_nodes,\n",
    "                                           embedding_dim,\n",
    "                                           embeddings_initializer=\"RandomNormal\",\n",
    "                                           input_length=1,\n",
    "                                           name=\"context_embedding\")\n",
    "        self.dots = Dot(axes=1)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair[:,0], pair[:,1]\n",
    "        word_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        dots = self.dots([context_emb, word_emb])\n",
    "        flat = self.flatten(dots)\n",
    "        return self.dense(flat)\n",
    "    \n",
    "    def summary(self):\n",
    "        x = Input(shape=(2,))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa7cbaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31385"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5efc78de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ronan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ronan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:189: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ronan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is a small graph, so the embedding dimensionality can be also small \n",
    "embedding_dim = 10\n",
    "nr_nodes = max(nodes)+1\n",
    "n2v = Node2Vec(nr_nodes=nr_nodes, embedding_dim=embedding_dim)\n",
    "n2v.build(input_shape=(nr_nodes, embedding_dim))\n",
    "n2v.compile(optimizer='adam',\n",
    "                 loss= 'binary_crossentropy', \n",
    "                 metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ffbc356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 2)]                  0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  (None,)                      0         ['input_1[0][0]']             \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None,)                      0         ['input_1[0][0]']             \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " context_embedding (Embeddi  (None, 10)                   313860    ['tf.__operators__.getitem_1[0\n",
      " ng)                                                                ][0]']                        \n",
      "                                                                                                  \n",
      " node_embedding (Embedding)  (None, 10)                   313860    ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dot (Dot)                   (None, 1)                    0         ['context_embedding[0][0]',   \n",
      "                                                                     'node_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 1)                    0         ['dot[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    2         ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 627722 (2.39 MB)\n",
      "Trainable params: 627722 (2.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n2v.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffdb0a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\ronan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "20000/20000 [==============================] - 17s 833us/step - loss: 0.5826 - accuracy: 0.7132 - auc: 0.6667\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 17s 830us/step - loss: 0.4549 - accuracy: 0.8101 - auc: 0.7700\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 17s 854us/step - loss: 0.4030 - accuracy: 0.8429 - auc: 0.8142\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 17s 863us/step - loss: 0.3781 - accuracy: 0.8575 - auc: 0.8364\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 16s 815us/step - loss: 0.3578 - accuracy: 0.8681 - auc: 0.8562\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 17s 871us/step - loss: 0.3391 - accuracy: 0.8758 - auc: 0.8785\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 16s 776us/step - loss: 0.3257 - accuracy: 0.8822 - auc: 0.8994\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 16s 793us/step - loss: 0.3165 - accuracy: 0.8862 - auc: 0.9118\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 16s 819us/step - loss: 0.3106 - accuracy: 0.8863 - auc: 0.9231\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 16s 799us/step - loss: 0.3070 - accuracy: 0.8881 - auc: 0.9335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d1a879ca50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2v.fit(gen, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95807686",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59987c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = n2v.get_layer('node_embedding').get_weights()[0]\n",
    "df = pd.DataFrame(embeddings)\n",
    "df.to_csv('new_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05aa195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = {idx: emb for idx, emb in enumerate(embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78857d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e11eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "explicit_map_dim = 100\n",
    "poly_ts = PolynomialCountSketch(degree=2, gamma=10, random_state=1, n_components=explicit_map_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf99cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph emebddings using explicit feature maps for the polynomial kernel.\n",
    "X = []\n",
    "for V in Vs:\n",
    "    graph_emb = np.array([.0 for _ in range(explicit_map_dim)])\n",
    "    for v in V:\n",
    "        emb_v = poly_ts.fit_transform([node_embeddings[v]])\n",
    "        graph_emb = np.add(graph_emb, emb_v[0])\n",
    "    graph_emb = graph_emb/len(V)\n",
    "    X.append(graph_emb)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3c9ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07daf9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8164784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, classes, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1afe9210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e40b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict_proba(X_test)[:,1]\n",
    "    print(\"AUC score: \", auc(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d76ab6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = LogisticRegression(C=1)\n",
    "model_rf = RandomForestClassifier(max_depth=12, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ff2fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.5229435011709602\n"
     ]
    }
   ],
   "source": [
    "# No luck with Logistic Regression\n",
    "train_and_evaluate(model_logreg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34e4c05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score:  0.8749268149882904\n"
     ]
    }
   ],
   "source": [
    "# The random forest classifier looks much better\n",
    "train_and_evaluate(model_rf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2df8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
